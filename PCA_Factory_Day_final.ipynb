{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4449ca68",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee3435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45b2e7",
   "metadata": {},
   "source": [
    "## Visualizing the Demo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6124e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demo data\n",
    "# DO NOT CHANGE\n",
    "X1 = np.random.normal(10, 5, 500)\n",
    "X2 = np.random.normal(0, .75, 500)\n",
    "data = np.vstack((X1, X2))\n",
    "th = 27 * np.pi/100\n",
    "R = np.array([[np.cos(th), -np.sin(th)],[np.sin(th), np.cos(th)]])\n",
    "data = (R @ data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cab1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "# TODO: plot the data variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64303f6",
   "metadata": {},
   "source": [
    "## Compute the Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd6920",
   "metadata": {},
   "source": [
    "$X$ is a matrix of data with $m$ rows and $n$ columns. Let $x^{(i)}$ denote the $i^{\\text{th}}$ column of $X$. Let $x^{(j)}$ denote the $j^{\\text{th}}$ column of $X$. The $(i,j)$ cell of the covariance matrix is computed as follows:\n",
    "$$\\text{cov}(X)_{i,j} = \\frac{1}{m-1}\\sum_{k=1}^{m}\\left[\\left(x_k^{(i)} - \\bar{x}^{(i)}\\right) \\left(x_k^{(j)} - \\bar{x}^{(j)}\\right) \\right]$$\n",
    "You can normalize the matrix $X$ by subtracting the mean from each column using the following command: `X_meaned = X - np.mean(X , axis = 0)`\n",
    "\n",
    "\n",
    "When $X$ is normalized like this, the covariance formula can be simplified as follows:\n",
    "$$\\text{cov}(\\bar{X})_{i,j} = \\frac{1}{m-1}\\sum_{k=1}^{m} x_k^{(i)} x_k^{(j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9788b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cov(X):\n",
    "    '''\n",
    "    Input: matrix X of size m x n\n",
    "    Output: covariance matrix of size n x n\n",
    "    \n",
    "    The (i,j) cell of the output is computed as follows:\n",
    "    '''\n",
    "    # Step 1: Normalize X using the command given above\n",
    "\n",
    "    \n",
    "    # Step 2: Compute the covariance matrix using the second formula above\n",
    "    \n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9d683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your covariance function\n",
    "# DO NOT CHANGE\n",
    "\n",
    "X = np.random.rand(30, 100)\n",
    "test = cov(X)\n",
    "X = X - np.mean(X, axis = 0)\n",
    "true = np.cov(X, rowvar=False)\n",
    "np.allclose(true, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc662e",
   "metadata": {},
   "source": [
    "## Vector Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9300612",
   "metadata": {},
   "source": [
    "Let $x_1$ and $x_2$ be $n$-dimensional vectors. The projection coordinates of $x_1$ onto $x_2$ is computed as follows:\n",
    "$$\\frac{x_1 \\cdot x_2}{\\lVert x_2 \\rVert}$$\n",
    "The Python documentation for computing the norm of a vector can be found here: https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85506cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vec_proj(x1, x2):\n",
    "    # Step 1: compute dot product of x1 and x2\n",
    "\n",
    "    # Step 2: divide outcome by norm of x2 (norm is taken with respect to columns -- axis=0)\n",
    "\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebec270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE\n",
    "\n",
    "x1 = np.array([5,6])\n",
    "x2 = np.array([10,3])\n",
    "V = np.array([x1, x2])\n",
    "origin = np.array([[0, 0],[0, 0]]) # origin point\n",
    "\n",
    "print(\"Before projection:\")\n",
    "plt.quiver(*origin, V[:,0], V[:,1], color=['r','k'], scale=10)\n",
    "plt.xlim((-0.01, 10))\n",
    "plt.ylim((-0.01, 8))\n",
    "plt.show()\n",
    "\n",
    "proj_x1 = vec_proj(x1, x2) * x2 / np.linalg.norm(x2)\n",
    "\n",
    "V = np.array([x1, x2, proj_x1])\n",
    "origin = np.array([[0, 0, 0],[0, 0, 0]]) # origin point\n",
    "\n",
    "print(\"After projection:\")\n",
    "plt.quiver(*origin, V[:,0], V[:,1], color=['r','k', 'g'], scale=10)\n",
    "plt.xlim((-0.01, 10))\n",
    "plt.ylim((-0.01, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a59c01",
   "metadata": {},
   "source": [
    "## Write Singular Value Decomposition (SVD) function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c3dfc",
   "metadata": {},
   "source": [
    "See the Python documentation for computing eigenvalues and eigenvectors: https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html\n",
    "\n",
    "You can also use this function:\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c2d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SVD(X , n):\n",
    "    '''\n",
    "    Input: X -- a matrix with m rows and n columns\n",
    "    Input: n -- the number of principal components to use\n",
    "    Output: project_X \n",
    "    Output: sorted_eigenvalues\n",
    "    Output: sorted_eigenvectors\n",
    "    '''\n",
    "     \n",
    "    #Step-1: normalize (subtract mean from each column using the line of code given above)\n",
    "     \n",
    "    #Step-2: compute covariance matrix\n",
    "     \n",
    "    #Step-3: obtain eigenvalues and eigenvectors\n",
    "\n",
    "    #Step-4: sort eigenvalues and eigenvectors in descending order\n",
    "    # Hint: use np.argsort(...)\n",
    "     \n",
    "    #Step-5: select the first n principal components (select the first n sorted eigenvectors)\n",
    "     \n",
    "    #Step-6: get the projection coordinates of the meaned data matrix onto the principal component subspace\n",
    "\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcdee7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVD test function\n",
    "# DO NOT CHANGE\n",
    "\n",
    "from numpy.linalg import svd\n",
    "n_cols = 5\n",
    "X = np.random.rand(500,n_cols)\n",
    "projected_X, sorted_vals, sorted_vecs = SVD(X, n_cols)\n",
    "X = X - np.mean(X , axis = 0)\n",
    "U, S, V = svd(X, full_matrices=False)\n",
    "projected_X_true = U @ np.diag(S[:])\n",
    "assert(np.allclose(np.abs(projected_X), np.abs(projected_X_true), atol=1e-8))\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab686af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize principal components\n",
    "# DO NOT CHANGE\n",
    "\n",
    "X_meaned = data - np.mean(data , axis = 0)\n",
    "cov_mat = np.cov(X_meaned, rowvar=False)\n",
    "\n",
    "eigenvalues , eigenvectors = np.linalg.eigh(cov_mat)\n",
    "\n",
    "v1 = eigenvectors[:,1]\n",
    "v2 = eigenvectors[:,0]\n",
    "\n",
    "\n",
    "m1 = v1[1] / v1[0]\n",
    "m2 = v2[1] / v2[0]\n",
    "xvals1 = np.linspace(0,8,100)\n",
    "xvals2 = np.linspace(0,3,100)\n",
    "\n",
    "\n",
    "plt.plot(X_meaned[:,0], X_meaned[:,1], 'r.', label='data')\n",
    "plt.plot(xvals1, xvals1*m1, 'k', linewidth=4, label='1st singular vector')\n",
    "plt.plot(xvals2, xvals2*m2, 'b', linewidth=4, label='2nd singular vector')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893389d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Visualizing singular vectors\n",
    "## DO NOT CHANGE\n",
    "f = lambda x: 15*np.exp(-x/20.0) + np.exp(x/40.0) - np.exp((x-75)/20.0) + np.exp((x-125)/15.0) + np.exp((x-150)/7.0)\n",
    "X = []\n",
    "for i in range(60):\n",
    "    row = []\n",
    "    for j in range(180):\n",
    "        row.append(f(j) + (1-2*np.random.rand()/6))\n",
    "    X.append(np.random.rand() * np.array(row) + 0.2*np.random.rand())\n",
    "X = np.array(X)\n",
    "projected_X, sorted_vals, sorted_vecs = SVD(X.T, 1)\n",
    "plt.plot(X.T)\n",
    "plt.plot(.25*projected_X, 'k', label='first singular vector')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25449431",
   "metadata": {},
   "source": [
    "## Applications of Singular Values - Explaining Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113855d",
   "metadata": {},
   "source": [
    "Singular values can be used to determine how much of the variance of the original dataset is explained by a low-dimensional approximation\n",
    "\n",
    "Assume the singular values are sorted as follows: $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_n$. The variance explained by a $k$-dimensional approximation is computed as follows:\n",
    "$$\\text{variance explained} = \\frac{\\sum\\limits_{i=1}^{k} \\sigma_{i}}{\\sum\\limits_{i=1}^{n} \\sigma_{i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36307e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def variance_explained(X, k):\n",
    "    '''\n",
    "    Input: X  -- A matrix with m rows and n columns\n",
    "    Input: k  -- (integer) the number of singular values to be used in the low-dimensional SVD approximation\n",
    "    Output: the percentage of the variance explained by a k-dimensional SVD approximation\n",
    "    '''\n",
    "    # Step 1: Compute SVD of X\n",
    "    \n",
    "    # Step 2: Compute numerator of the fraction above\n",
    "    \n",
    "    # Step 3: Compute denomenator of the fraction above\n",
    "    \n",
    "    # Step 4: Return numerator / denomenator\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c0242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applying the PCA function\n",
    "# DO NOT CHANGE\n",
    "\n",
    "n_cols = 50\n",
    "X = np.random.rand(500,n_cols)\n",
    "\n",
    "# eigen_val_sum = np.sum(eigenvalues)\n",
    "# total_of_eigen_vals = 0\n",
    "percent_var_explained = []\n",
    "for i in range(1, n_cols+1):\n",
    "    percent_var_explained.append(variance_explained(X, i))\n",
    "\n",
    "plt.plot(range(len(percent_var_explained) + 1), [0] + percent_var_explained)\n",
    "plt.xlabel('Number of singular values used in PCA')\n",
    "plt.ylabel('Percent of variance explained')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(percent_var_explained) + 1), percent_var_explained)\n",
    "plt.xlabel('Number of singular values used in PCA')\n",
    "plt.ylabel('Percent of variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3634ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Do this again with Iris dataset\n",
    "# DO NOT CHANGE\n",
    "\n",
    "#Load the IRIS dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "data = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n",
    "#prepare the data\n",
    "X = data.iloc[:,0:4]\n",
    "\n",
    "# eigen_val_sum = np.sum(eigenvalues)\n",
    "# total_of_eigen_vals = 0\n",
    "percent_var_explained = []\n",
    "for i in range(1, X.shape[1]+1):\n",
    "    percent_var_explained.append(variance_explained(X, i))\n",
    "\n",
    "plt.plot(range(len(percent_var_explained) + 1), [0] + percent_var_explained)\n",
    "plt.xlabel('Number of singular values used in PCA')\n",
    "plt.ylabel('Percent of variance explained')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(percent_var_explained) + 1), percent_var_explained)\n",
    "plt.xlabel('Number of singular values used in PCA')\n",
    "plt.ylabel('Percent of variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919da2a",
   "metadata": {},
   "source": [
    "## Applications of Singular Values - Matrix Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b4f7b",
   "metadata": {},
   "source": [
    "Let $X$ denote a matrix containing data with $m$ rows and $n$ columns. The traditional SVD yields a decomposition of the form\n",
    "$$X = U \\Sigma V^T$$\n",
    "where\n",
    "$$U \\in \\mathbb{R}^{m \\times n}$$\n",
    "$$\\Sigma \\in \\mathbb{R}^{n \\times n}$$\n",
    "$$V^{T} \\in \\mathbb{R}^{n \\times n}$$\n",
    "These three matrices can be obtained using the following line of code: `U, S, VT = svd(X, full_matrices=False)`\n",
    "\n",
    "By default, $S$ will be a vector. If you want to do a $k$-dimensional approximation of $X$, only use the first $k$ values of $S$ and zero out the rest. For example, $S = [\\sigma_1, \\sigma_2, \\sigma_3, ...\\sigma_n]$. The $S$ you want for a $k$-dimensional approximation is $S_k = [\\sigma_1, \\sigma_2, \\dots, \\sigma_k, 0, 0, \\dots, 0]$ where $\\sigma_{k+1}, \\sigma_{k_2},\\dots,\\sigma_n$ have been replaced by 0s. \n",
    "\n",
    "Then, turn the vector $S$ into matrix form by `Sk = np.diag(S)`.\n",
    "\n",
    "The $k$-dimensional approximation is computed by $ U S_k V^{T} $.\n",
    "\n",
    "Finally, the error of this approximation can be computed by $\\lVert X - U S_k V^{T} \\rVert$. See https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html for an example of how to use the norm function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c53569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def approximation_error(X, k):\n",
    "    '''\n",
    "    Input: X  -- A matrix with m rows and n columns\n",
    "    Input: k  -- (integer) the number of singular values to be used in the low-dimensional SVD approximation\n",
    "    Output: (float) the error of the reconstructed X matrix \n",
    "    '''\n",
    "    \n",
    "    # Step 1: compute SVD of X using numpy svd to obtain U, S, and VT (set full_matrices=False)\n",
    "\n",
    "    # Step 2: zero out the elements of S after the k-th singular value\n",
    "\n",
    "    # Step 3: Convert S to a diagonal matrix\n",
    "\n",
    "    # Step 4: compute the reduced rank approximation of X\n",
    "\n",
    "    # Step 5: compute the norm of the difference between X and your reduced rank approximation of X\n",
    "    \n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fb6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_cols = 100\n",
    "X = np.random.rand(1000,n_cols)\n",
    "errors = []\n",
    "\n",
    "for i in range(int(np.floor(.9 * n_cols))):\n",
    "    errors.append(approximation_error(X, i))\n",
    "\n",
    "    \n",
    "plt.plot(np.log10(errors))\n",
    "plt.xlabel('Number of singular values used in PCA')\n",
    "plt.ylabel('Reconstruction error (log10)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdc5a1",
   "metadata": {},
   "source": [
    "## Perform PCA on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007323b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the IRIS dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "data = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n",
    "#prepare the data\n",
    "X = data.iloc[:,0:4]\n",
    "#prepare the labels\n",
    "labels = data.iloc[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61644f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_df = pd.DataFrame(X.iloc[:, 0:2])\n",
    "initial_df = pd.concat([initial_df , pd.DataFrame(labels)] , axis = 1)\n",
    "initial_df.columns = ['coord1', 'coord2', 'classes']\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "# sb.scatterplot(data=initial_df , x='coord1', y='coord2' , hue='classes' , s=60 , palette='icefire')\n",
    "sb.scatterplot(data=initial_df , x='coord1', y='coord2', s=60)\n",
    "# plt.plot(X[:,0], X[:,1], 'k.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25062c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating a Pandas DataFrame of reduced dataset\n",
    "\n",
    "# Use your SVD function with 2 principal components\n",
    "mat_reduced, sorted_eigenvalues, sorted_eigenvectors = SVD(X, 2)\n",
    "\n",
    "# Create a pandas dataframe using the reduced matrix from the output of your SVD function. Label the columns 'PC1' and 'PC2'\n",
    "principal_df = pd.DataFrame(mat_reduced, columns=['PC1', 'PC2'])\n",
    "\n",
    "#Concat it with target variable to create a complete Dataset\n",
    "principal_df = pd.concat([principal_df , pd.DataFrame(labels)] , axis = 1)\n",
    "\n",
    "principal_df.columns = ['PC1', 'PC2', 'classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b3b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "sb.scatterplot(data = principal_df , x = 'PC1',y = 'PC2' , hue = 'classes' , s = 60 , palette= 'icefire')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8456940",
   "metadata": {},
   "source": [
    "## PCA with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7ffb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.iloc[:,0:4]\n",
    "labels = data.iloc[:,4]\n",
    "\n",
    "# Engineer a new feature by squaring one of the columns of X\n",
    "\n",
    "# Use your SVD function to perform PCA\n",
    "mat_reduced, eigenvalues, eigenvectors = ...\n",
    "\n",
    "principal_df_3_components = pd.DataFrame(mat_reduced , columns = ['PC1', 'PC2', 'PC3'])\n",
    " \n",
    "#Concat it with target variable to create a complete Dataset\n",
    "principal_df = pd.concat([principal_df_3_components , pd.DataFrame(labels)] , axis = 1)\n",
    "\n",
    "principal_df.columns = ['PC1', 'PC2', 'PC3', 'classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc10d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a train/test split using test_size=0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "# Print the unique labels\n",
    "print(...)\n",
    "\n",
    "# Create training matrices for each class\n",
    "X1 = ...\n",
    "X2 = ...\n",
    "X3 = ...\n",
    "\n",
    "# The number of classes is equal to the number of unique labels\n",
    "num_classes = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaebe28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thetas = np.linspace(0, 2*np.pi, num_classes + 1)[0:num_classes]\n",
    "Y = np.vstack((np.cos(thetas), np.sin(thetas))).T\n",
    "targets = np.zeros((len(y_train), 2))\n",
    "# Set the mapping destinations\n",
    "targets[...] = ...\n",
    "targets[...] = ...\n",
    "targets[...] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a9963",
   "metadata": {},
   "source": [
    "The least squares documentation is here: https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\n",
    "\n",
    "The output of the least squares function will be a list with several elements. The mapping we are looking for is in the 0th element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281f447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use least squares to create a mapping from the representatives to locations in the plane\n",
    "mapping = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f63e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the supervised approximation using the mapping\n",
    "approx = X_test @ mapping\n",
    "lda_test_df = pd.DataFrame(np.column_stack([approx[:,0],approx[:,1], y_test]), columns=['LD1','LD2','class'])\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "sb.scatterplot(data = lda_test_df , x = 'LD1',y = 'LD2' , hue = 'class' , s = 60 , palette= 'icefire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the supervised approximation using the mapping\n",
    "approx = mat_reduced @ mapping\n",
    "lda_test_df = pd.DataFrame(np.column_stack([approx[:,0],approx[:,1], labels]), columns=['LD1','LD2','class'])\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "sb.scatterplot(data = lda_test_df , x = 'LD1',y = 'LD2' , hue = 'class' , s = 60 , palette= 'icefire')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
